import numpy as np
import pandas as pd
import pickle
import streamlit as st
import regex
from underthesea import word_tokenize, pos_tag, sent_tokenize
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from streamlit_folium import folium_static
import folium
import time
import json
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import traceback
from PIL import Image
from io import StringIO

########################################################################

positive_words = [
    "th√≠ch", "t·ªët", "xu·∫•t s·∫Øc", "tuy·ªát v·ªùi", "tuy·ªát h·∫£o", "ƒë·∫πp", "·ªïn", "ngon",
    "h√†i l√≤ng", "∆∞ng √Ω", "ho√†n h·∫£o", "ch·∫•t l∆∞·ª£ng", "th√∫ v·ªã", "nhanh", "ƒë√∫ng"
    "ti·ªán l·ª£i", "d·ªÖ s·ª≠ d·ª•ng", "hi·ªáu qu·∫£", "·∫•n t∆∞·ª£ng",
    "n·ªïi b·∫≠t", "t·∫≠n h∆∞·ªüng", "t·ªën √≠t th·ªùi gian", "th√¢n thi·ªán", "h·∫•p d·∫´n",
    "g·ª£i c·∫£m", "t∆∞∆°i m·ªõi", "l·∫° m·∫Øt", "cao c·∫•p", "ƒë·ªôc ƒë√°o",
    "h·ª£p kh·∫©u v·ªã", "r·∫•t t·ªët", "r·∫•t th√≠ch", "t·∫≠n t√¢m", "ƒë√°ng tin c·∫≠y", "ƒë·∫≥ng c·∫•p",
    "h·∫•p d·∫´n", "an t√¢m", "kh√¥ng th·ªÉ c∆∞·ª°ng l·∫°i", "th·ªèa m√£n", "th√∫c ƒë·∫©y",
    "c·∫£m ƒë·ªông", "ph·ª•c v·ª• t·ªët", "l√†m h√†i l√≤ng", "g√¢y ·∫•n t∆∞·ª£ng", "n·ªïi tr·ªôi",
    "s√°ng t·∫°o", "qu√Ω b√°u", "ph√π h·ª£p", "t·∫≠n t√¢m", 'ƒë·ªìng_√Ω',
    "hi·∫øm c√≥", "c·∫£i thi·ªán", "ho√† nh√£", "chƒÉm ch·ªâ", "c·∫©n th·∫≠n",
    "vui v·∫ª", "s√°ng s·ªßa", "h√†o h·ª©ng", "ƒëam m√™", "v·ª´a v·∫∑n", "ƒë√°ng ti·ªÅn", "x·ª©ng ƒë√°ng", "ƒë·∫ßy ƒë·ªß", "ƒë·ªß",'·ªïn', 'k·ªπ', 'g·∫ßn', 'vui v·∫ª', 'h√†i l√≤ng', 'y√™n t√¢m', 'r·∫ª', 'r√µ r√†ng', 'c·∫©n th·∫≠n', 'dc', 'th∆°m',
    'xu·∫•t s·∫Øc', 'tho·∫£i m√°i', 'n√≥ng h·ªïi', 'ƒë·∫∑c bi·ªát', 'ƒë·∫πp', 'r·ªông r√£i', 's·∫°ch s·∫Ω', 'hi·ªán ƒë·∫°i', 'm·∫°nh', 't·ªët', 'ch·∫Øc ch·∫Øn', 'ƒë·ªÅu ƒë·∫∑n', 'nhanh', 'ti·ªán l·ª£i', '·ªïn ƒë·ªãnh', 'r·ªông l·ªõn', 'm√°t', 'ok', 'nhi·ªát t√¨nh', 'l·∫π',
    'ni·ªÅm n·ªü', 'xinh x·∫Øn', 't·ª± nhi√™n', 'chuy√™n nghi·ªáp', 'l·ªãch s·ª±', 'ƒë√†ng ho√†ng', 'thu·∫≠n l·ª£i', '·∫•n t∆∞·ª£ng', 'ti·ªán', 'ƒëa d·∫°ng', 'tho√°ng m√°t', 'ƒë·∫πp m·∫Øt', 'h·ª£p l√Ω', 'd·ªÖ th∆∞∆°ng', 'tuy·ªát v·ªùi', '·∫•m c√∫ng',
    'd·ªãu', 'nh·∫π', 'xinh g√°i', 't·ª≠ t·∫ø', 'nh·∫π nh√†ng', 'nhanh nh·∫πn', 'nhanh ch√≥ng', 'g·ªçn', 'ƒë·∫≠m ƒë√†', 'm·ªã th√≠t', 'th∆°m ng·∫≠y', 'ƒë·∫∑c s·∫Øc', '∆∞ng', 'n·ªïi b·∫≠t', 'x·ªãn', 'tho√°ng', 'ƒÉn ok', 's∆∞·ªõng', 'kh·ªèi ph·∫£i n√≥i', 'ho√†nh tr√°ng',
    '·∫•m', 'no √°', 't·∫≠n t√¨nh', 'nhah', 'l·∫°', 'ƒë√¥ng ƒë√∫c', 'ƒë·∫Øc ƒë·ªãa', 'b√¨nh d√¢n', 'chuy·ªán nghi·ªáp', 'b·∫Øt v·ªã', 't·∫•p n·∫≠p', 'mau', 'ƒë·∫≠m v·ªã', 'gi√≤n nha', 'd·ªÖ ch·ªãu', 'ko b·ªü', 'b·ª±', 'd·ªÖ t√≠nh', 'ƒë·∫ßy ƒë·∫∑n', 'h·∫•p d·∫´n', 'ngon mi·ªáng', 'm√°t m·∫ª',
    'chi√™n gi√≤n', 'tuy·ªát', 's·∫°ch', 'mi·ªát m√†i', 'th√≠ch v·ªã', 'gi√≤n r·ª•m', 'c·∫£m k√≠ch', 'ng·ªçt nha', 's√°ng s·ªßa', 't∆∞∆°i ngon',
    'l·ªÖ ph√©p', 'g·ªçn g√†ng', 'ƒë·∫ßy ·∫Øp', '·ªïn √°p', 'ch·ªân chu', 'si√™u to', 'kh·ªïng l·ªì', 'ngon l√†nh', 'kha kh√°', 'th·ªèa m√£n', 'phi th∆°m', 'chu ƒë√°o', 'an to√†n', 'k·ªπ l∆∞·ª°ng', 'nh·ªôn nh·ªãp', 'v·ª´a ph·∫£i', 'b√∫n ngon', 'b√∫n ƒë·∫πp', 'quen thu·ªôc',
    'ngon m·∫Øt', '∆∞ng qu√°n', 'l·∫° mi·ªáng', 'an t√¢m', 'ch·∫•t l∆∞·ª£ng', 'g·∫ßn g≈©i', 'tuy·ªát', 't·∫≠n tay', 'bbq th∆°m', 'm·ªÅm th∆°m', 'x·ªëp gi√≤n', 'ph·∫£i chƒÉng', 'chu·∫©n', 'salad t∆∞∆°i', 'h·ª£p', 'tuy·ªát h∆∞∆°ng v·ªã',
    'no no', 'ho√†n ch·ªânh', 'ƒë∆∞·ª£m v·ªã', 'th∆∞·ª£ng h·∫°ng', 'ƒÉn h·ª£p', 't·ªët nha', 'ƒë·ªôc ƒë√°o', 'xinh', 'b·∫Øt m·∫Øt', '·ªïn hen', 'r·ªôp r·ªôp', '∆∞u ƒëi·ªÉm', 'gi·∫£i nhi·ªát',
    'ok nha', 'ch√≠nh x√°c', 'h·ª£p mi·ªáng', 'ngon m·ªÅm', 'h√∫t h·ªìn', 'h√≤a quy·ªán', 'ho√†n h·∫£o', 'kh√°', 'ngon nhaaa', 'h·ª£p v·ªã', 'm√°t n√®', 'l√£ng m·∫°n', 'th√¥ng tho√°ng', 'ch·ªânh chu', 'v·ª´a v·ªã', 'm·ªõi l·∫°',
    'tr·ªçn v·∫πn', 'x·ªãn s√≤', 'd·ªÖ ƒÉn', 'sang tr·ªçng', '√¢n c·∫ßn', 'ngon n√®', 'gi√≤n tan', 'y√™n tƒ©nh', 'thanh b√¨nh', 'th√≠ch h·ª£p', 'oke', 'ngon nha', 'nice', 'm·ªÅm m·∫°i', '√° ngon', 'si√™u ngon', 'kho√°i',
    '∆∞u ti√™n', 'th√¥ng minh', 'ƒÉn th∆°m', 'n∆∞·ªùm n∆∞·ª£p', 'n·ªïi danh', 'v·ª´a √Ω', 'g√† gi√≤n', 'cute', 'h√†o h·ª©ng', 's√†nh ƒëi·ªáu', 'ƒë·ªânh', 'cuteee', 'ghi·ªÅn', 'ƒë·∫≠m n√©t', 'lan t·ªèa', 'kh√©o l√©o', 'c·∫£m t√¨nh',
    '∆∞ng √Ω', 't∆∞∆°m t·∫•t', 'nhi·ªát huy·∫øt', 'thu·∫≠n ti·ªán', 'b√°t m·∫Øt', 'ƒëi·ªÉm c·ªông', 'good', 'xinh xinh', 'tuy·ªát lu√¥n', 'gi·ªèi', 'th∆°m l·ª´ng', 't∆∞∆°i xanh', 'ngon c·ª±c', 'ngon ƒë·∫∑c s·∫Øc', 'ni·ªÅm n·ª°',
    'best', 'ƒÉn b√° ch·∫•y', 'ƒÉn ngon', 'th∆°m th∆°m', 'th√¢n th∆∞∆°ng', 'mi·ªÖn ch√™', 'th√≠ch th√∫', 'ngon l√¢u', 'n√°o nhi·ªát', 'ƒë·∫≥ng c·∫•p', 'to·∫πt v·ªùi', 'th·ªèa s·ª©c', 'cu·ªëng', 'd·ªÖ th∆∞∆°ng nha', 'bao ph√™', 'm√°t l·∫°nh',
    'v∆∞·ª£t tr·ªôi', 'xu·∫•t x·∫Øc', 'ngon t·∫πo', 'm√°t r∆∞·ª£i', 'b·ªï d∆∞·ª°ng', 'vui nh·ªôn', 'ƒÉn m√™', '√¥ k√™', 'ph√°i', 'ng·∫≠p m·∫∑t', 'kƒ© nh√©', 'sinh ƒë·ªông', 'c∆∞nng xƒ©u', 'si√™u h·ª£p', 'ti·ªán nghi', 'b√°n ch·∫°y',
    'tuy√™t v·ªùi', 'ngon b·ªï', 'tr√°i c√¢y ƒÉn t∆∞∆°i', 'duoc', 'tinh khi·∫øt', 'si√™u x·ªãn', 'ƒë·ªì s·ªô'
]

negative_words = [
    "k√©m", "t·ªá", "ƒëau", "x·∫•u", "d·ªü", "·ª©c", "nh·ªè"
    "bu·ªìn", "r·ªëi", "th√¥", "l√¢u", "ch√°n"
    "t·ªëi", "ch√°n", "√≠t", "m·ªù", "m·ªèng",
    "l·ªèng l·∫ªo", "kh√≥", "c√πi", "y·∫øu",
    "k√©m ch·∫•t l∆∞·ª£ng", "kh√¥ng th√≠ch", "kh√¥ng th√∫ v·ªã", "kh√¥ng ·ªïn",
    "kh√¥ng h·ª£p", "kh√¥ng ƒë√°ng tin c·∫≠y", "kh√¥ng chuy√™n nghi·ªáp",
    "kh√¥ng ph·∫£n h·ªìi", "kh√¥ng an to√†n", "kh√¥ng ph√π h·ª£p", "kh√¥ng th√¢n thi·ªán", "kh√¥ng linh ho·∫°t", "kh√¥ng ƒë√°ng gi√°",
    "kh√¥ng ·∫•n t∆∞·ª£ng", "kh√¥ng t·ªët", "ch·∫≠m", "kh√≥ khƒÉn", "ph·ª©c t·∫°p",
    "kh√≥ hi·ªÉu", "kh√≥ ch·ªãu", "g√¢y kh√≥ d·ªÖ", "r∆∞·ªùm r√†", "kh√≥ truy c·∫≠p",
    "th·∫•t b·∫°i", "t·ªìi t·ªá", "kh√≥ x·ª≠", "kh√¥ng th·ªÉ ch·∫•p nh·∫≠n", "t·ªìi t·ªá","kh√¥ng r√µ r√†ng",
    "kh√¥ng ch·∫Øc ch·∫Øn", "r·ªëi r·∫Øm", "kh√¥ng ti·ªán l·ª£i", "kh√¥ng ƒë√°ng ti·ªÅn", "ch∆∞a ƒë·∫πp", "kh√¥ng ƒë·∫πp", "tanh", "l√¢u",'d·ªü', 'kh√≥ ch·ªãu', 'ngang', 't·ªá', 'mau ng√°n', 'thi·∫øu', '√≠t', 'b·ªü', 'c·∫©u th·∫£', 'b√¨nh th∆∞·ªùng', 'ng·∫≠p ng·ª•a', 'ngh√®o',
    't·ªüm l·ª£m', 'nh·ªè x√≠u', 'h·∫±n h·ªçc', 'l∆∞·ªùi', 'k√©m', 'c·ª£t nh√£', 'm·∫∑n', 'v·∫Øng', 'ngu·ªôi', '·ªìn √†o', '·ªìn √≠', 'th√∫ v·ªã', 'b·∫•t ti·ªán', 'gh√™', 'ƒë·∫Øt', 'kh√≥', 'gi√†', 'nh·∫°t', 'l·∫°nh', 'nh·∫°t nh·∫Ωo', 'h·ªèng', 'x·∫•c x∆∞·ª£c', 'b·ª±c m√¨nh',
    'l∆° ng∆°', 'n·ªìng', 'm√π', 'nhanh ng·∫•y', 'd∆°', 'h√¥i', 'ng√°n', 's·ªëc', 'kh√≥ t√≠nh', '·ª©c ch·∫ø', 'qu√° k√¨', 'ch·∫≠t', 'h∆∞', 'b·∫•t x√∫c', 'ng·ªôp', 'nghi ng√∫t', 'l√¢u l√¢u', 'ch·ª© t·ªá', 'th√∫i', 'bu·ªìn', 'kinh', 'dai', 't·ªá h·∫°i',
    'ch·∫≠t ch·ªôi', 'l√≤ng v√≤ng', 'b·ªë ƒë·ªùi', 'gh√™ g·ªõm', 'ƒë·∫Øt ƒë·ªè', 't·ª•t h·ª©ng', 'gi·∫£', 'k√¨ k√¨', 'ko h·ª£p', 's·∫ßn', 'h·∫Øc √°m', 'm·ªÅ', 'ko th∆°m', 'xui', 'ng·∫≠y ng·∫≠y', 'b·∫•t th∆∞·ªùng', 'review t·ªá', 'l·ªèng l·∫ªo', 'r√°ch n√°t',
    'h·ª•t h·∫´ng', 'ch·∫≠m', 'nh√£o', 'nh·ªè h·∫πp', 'h·∫πp', 'm·ªát', 'b·ª©c x√∫c', 'ko h·∫£o', 'n√°t', 'nh∆∞·ª£c ƒëi·ªÉm','v·ªôi', 'd·∫ßu m·ª°', 'l∆° l√†', 'g·ªõm', 'm·∫∑n ch√°t', 'm·∫•t h·ª©ng', 'l√≥ng ng√≥ng', 'm·ª°', 'ch·∫≠t h·∫πp', 'khinh th∆∞·ªùng', 'l·ªôn x·ªôn',
    'm√≤n m·ªèi', 'r√°ch', 'ng·ªôt ng·∫°t','c·ªôc l·ªëc', 'cua b·ªÉ', 'v√¥ l·ªÖ', 'c·ª±c t·ªá', 'u·ªÉ o·∫£i', 'b·∫•t l·ªãch s·ª±', 'dai', 'nhanh ng√°n', 'h·∫•p d·∫©n', 'v√¥ duy√™n', 'ngon ·∫°', 'g√≤ b√≥', 'th·ªëi', 'kh√¥ s√°p', 'ƒë·∫Øng', 'g√† h√¥i', 'b·∫•t h·ª£p l√Ω', 'x·∫•u', 'g·∫•p g√°p',
    'hƒÉng', 'tan n√°t', 's·ª£ h√£i', 'l√∫ng t√∫ng', 'h·ªÅ tanh', 'c≈© k·ªπ', '·ªìn ao', 'fail', 'nh·ª©c nh·ªëi', 'ch·∫£nh', 'l·ªìi l√µm', 'm·∫•t d·∫°y', 'th·∫•t v·ªçng x√≠u', 'gian d·ªëi', 'kh·ªßng khi·∫øp', 'ng√°n nh√¢n vi√™n', 'v·ª•ng v·ªÅ', 'cho√°ng',
    't·ªìi t·ªá', 'gi·ªÖu c·ª£t', 'v√¥ tr√°ch nhi·ªám', 'tr·∫ßy tr·∫≠t', 'thi·∫øu thi·ªán c·∫£m', 'ch·∫£nh ch·ªçe', 'kinh d·ªã', 'x√©o s·∫Øc', 'd∆° d∆°', 's∆∞·ª£ng', 'b·∫©n b·∫©n', 'ngu·ªách ngo·∫°c', 'ng√°o ng∆°'
]

## english-vnmese
file = open('files/english-vnmese.txt', 'r', encoding="utf8")
vn_lst = file.read().split('\n')
english_vnmese = {}
for line in vn_lst:
    key, value = line.split('\t')
    english_vnmese[key] = str(value)
file.close()

#LOAD TEENCODE
file = open('files/teencode.txt', 'r', encoding="utf8")
teen_lst = file.read().split('\n')
teen_dict = {}
for line in teen_lst:
    key, value = line.split('\t')
    teen_dict[key] = str(value)
file.close()

### vietnamese-stopwords
file = open('files/vietnamese-stopwords.txt', 'r', encoding="utf8")
stopwords_lst = file.read().split('\n')
file.close()

##LOAD EMOJICON
file = open('files/emojicon.txt', 'r', encoding="utf8")
emoji_lst = file.read().split('\n')
emoji_dict = {}
for line in emoji_lst:
    key, value = line.split('\t')
    emoji_dict[key] = str(value)
file.close()

#Load models 
model_pkl_file = 'model_rdf.pkl'

# List functions for handling data
def process_text(text: str, emoji_dict, teen_dict, english_vnmese):
  document = text.lower()
  document = document.replace("‚Äô",'')
  document = regex.sub(r'\.+', ".", document)
  new_sentence =''
  for sentence in sent_tokenize(document):
    sentence = regex.sub(r'(?<=[^\W\d_])\b', ' ', sentence)
    sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
    sentence = ' '.join(english_vnmese[word] if word in english_vnmese else word for word in sentence.split())
    sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
    ###### DEL Punctuation & Numbers
    pattern = r'(?i)\b[a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]+\b'
    sentence = ' '.join(regex.findall(pattern,sentence))
    new_sentence = new_sentence+ sentence + '. '
  document = new_sentence
  document = regex.sub(r'\s+', ' ', document).strip()
  return document

def loaddicchar():
    dic = {}
    char1252 = 'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'.split(
        '|')
    charutf8 = "√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

def covert_unicode(txt):
    dicchar = loaddicchar()
    return re.sub(
        r'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£',
        lambda x: dicchar[x.group()], txt)

def process_special_word(text):
  new_text = ''
  text_lst = text.split()
  i= 0
  if 'kh√¥ng' in text_lst:
    while i <= len(text_lst) - 1:
      word = text_lst[i]
      if word == 'kh√¥ng':
        next_idx = i+1
        if next_idx <= len(text_lst) -1:
          word = word +'_'+ text_lst[next_idx]
        i= next_idx + 1
      else:
        i = i+1
      new_text = new_text + word + ' '
  else:
    new_text = text
  return new_text.strip()

def normalize_repeated_characters(text):
  return re.sub(r'(.)\1+', r'\1', text)

def process_postag_thesea(text):
  new_document = ''
  for sentence in sent_tokenize(text):
    sentence = sentence.replace('.','')
    lst_word_type = ['N','Np','A','AB','V','VB','VY','R', 'M']
    sentence = ' '.join( word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
    new_document = new_document + sentence + ' '
  new_document = regex.sub(r'\s+', ' ', new_document).strip()
  return new_document

def remove_stopword(text, stopwords):
  document = ' '.join('' if word in stopwords else word for word in text.split())
  document = regex.sub(r'\s+', ' ', document).strip()
  return document

def find_words(document, list_of_words):
  document_lower = document.lower()
  word_count = 0
  for word in list_of_words:
    if word in document_lower:
      word_count += document_lower.count(word)
  return word_count


def extract_words(document, list_of_words):
  document_lower = document.lower()
  word_list = []
  for word in list_of_words:
    if word in document_lower:
      word_list.append(word)
  return word_list

######################################
def handle_comment(cmt):
    document = process_text(cmt, emoji_dict, teen_dict, english_vnmese)
    document = covert_unicode(document)
    document = process_special_word(document)
    document = normalize_repeated_characters(document)
    document = process_postag_thesea(document)
    document = remove_stopword(document, stopwords_lst)
    pos = find_words(document, positive_words)
    neg = find_words(document, negative_words)
    vectorizer = TfidfVectorizer(max_features=1000)
    comment = vectorizer.fit_transform([document])
    tfidf_array_full = np.zeros((comment.shape[0], 1000))
    tfidf_array_full[:, :comment.shape[1]] = comment.toarray()
    df_comment = pd.DataFrame(tfidf_array_full)
    df_comment['positive_words'] = pos
    df_comment['negative_words'] = neg
    df_comment.columns = df_comment.columns.astype(str)
    return df_comment

def predict(model_pkl_file, id = None, comment = None):
    df_new = pd.read_csv('files/matrix_comment.csv.gz', compression='gzip')
    cmt = None
    id_res = None
    with open(model_pkl_file, 'rb') as file:
        loaded_model = pickle.load(file)
    if id is not None and comment is None:
        if id in df_new['ID'].unique().tolist():
            id_res = df_new.loc[df_new['ID'] == id].drop(columns=['ID', 'Label'])
        else:
            print('Kh√¥ng c√≥ b√¨nh lu·∫≠n')
        prediction = loaded_model.predict(id_res)
    else:
        cmt = handle_comment(comment)
        prediction = loaded_model.predict(cmt)
    return prediction

def filter_adjectives(text) -> list:
    list_obj = []
    tags = pos_tag(text)
    for word, tag in tags:
        if tag.startswith('A'):
            if word.lower() not in list_obj:
                list_obj.append(word.lower())
    return list_obj

def get_id_input():
    id = None
    while id is None:
        id = st.number_input("ID Restaurant from 1 to 1621", min_value=1, max_value=1621, value = 15)
    return id

def handle_comment_(df_comment, id):
    df_pos_sort = None
    df_neg_sort = None
    comment_pos = []
    time_pos = []
    pos = []
    idx_pos = []
    comment_neg = []
    time_neg = []
    neg = []
    idx_neg = []
    try:
        list_index = df_comment[df_comment['IDRestaurant'] == id]['Comment'].index
        output = predict(model_pkl_file, id = id)
        for i, value in enumerate(output):
            if value == 0 or value == '0':
                if len(df_comment['Comment'][list_index[i]]) > 0:
                    comment_neg.append(df_comment['Comment'][list_index[i]])
                    time_neg.append(df_comment['Time'][list_index[i]])
                else:
                    comment_neg.append("")
                    time_neg.append('')
                neg.append("Ti√™u c·ª±c")
                idx_neg.append(list_index[i])
            else:
                if len(df_comment['Comment'][list_index[i]]) > 0:
                    comment_pos.append(df_comment['Comment'][list_index[i]])
                    time_pos.append(df_comment['Time'][list_index[i]])
                else:
                    comment_pos.append("")
                    time_pos.append('')
                pos.append("T√≠ch c·ª±c")
                idx_pos.append(list_index[i])

        result_pos = {
            "ID": idx_pos,
            "Comment": comment_pos,
            "Time": time_pos,
            "Predict": pos
        }

        df_pos = pd.DataFrame(result_pos)
        result_neg = {
            "ID": idx_neg,
            "Comment": comment_neg,
            "Time": time_neg,
            "Predict": neg
        }
        
        df_neg = pd.DataFrame(result_neg)
        df_pos_sort = df_pos.sort_values(by='Time', ascending=False)
        df_neg_sort = df_neg.sort_values(by='Time', ascending=False)
    except Exception as e:
        st.error(f"An error occurred: {e}")
        # Print the traceback
        traceback.print_exc()

    return df_pos_sort, df_neg_sort

def download_csv(df: pd.DataFrame, filename: str):
    # Create a string buffer
    buffer = StringIO()
    df.to_csv(buffer, index=False, encoding='utf-8')
    buffer.seek(0)
    st.download_button(
        label="Download File CSV",
        data=buffer.getvalue(),
        file_name=filename,
        mime="text/csv"
    )

#GUI
st.set_page_config(page_title='Data Science Final Project: Sentiment Analysis', page_icon=':fries' ,layout="wide",)
image = Image.open('logo.jpg')

st.title("Data Science Final Project: Sentiment Analysis üçî")
st.markdown("""
<style>
.stApp {
    background-color: #FFFFFF;
}
.css-1aumxhk {
    background-color: #ffe0e0;
    color: #ffffff;
}
</style>
""", unsafe_allow_html=True)
custom_css = """
    <style>
        .css-5sror9 {
            font-family: 'Roboto';
            font-weight: normal !important;
        }
    </style>
"""
sep_line = """
    <hr style="border-top: 0.1px solid #F0FFFF; margin-top: 1rem; margin-bottom: 1rem;" />
"""
st.markdown(custom_css, unsafe_allow_html=True)
menu = ["Gi·ªõi Thi·ªáu", "Th√¥ng Tin Thu·∫≠t To√°n", "ƒê√°nh Gi√° B√¨nh Lu·∫≠n","ƒê√°nh Gi√° Nh√† H√†ng"]
choice = st.sidebar.selectbox('Menu', menu)
st.subheader(f":green[{choice}]")

df = pd.read_csv('files/Restaurants.csv')
df_comment = pd.read_csv('files/2_Reviews.csv')

if choice == 'Gi·ªõi Thi·ªáu':  
    st.image(image, width = 450)
    st.markdown('**1. Gi·ªõi thi·ªáu v·ªÅ ·ª©ng d·ª•ng:**')
    st.info('·ª®ng d·ª•ng ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n thu·∫≠t to√°n x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) nh·∫±m x√°c ƒë·ªãnh xem ƒë√°nh gi√° l√† t√≠ch c·ª±c hay ti√™u c·ª±c. Ph√¢n t√≠ch th∆∞·ªùng ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n c√°c ph·∫£n h·ªìi, ƒë√°nh gi√°, kh·∫£o s√°t nh·∫±m ƒëo l∆∞·ªùng m·ª©c ƒë·ªô h√†i l√≤ng c·ªßa kh√°ch h√†ng ƒë·ªëi v·ªõi s·∫£n ph·∫©m v√† d·ªãch v·ª• c·ªßa c√¥ng ty.')

    st.markdown('**2. M·ª•c ti√™u ·ª©ng d·ª•ng mang l·∫°i:**')
    st.markdown('''
    <div style="background-color: #FAF3F0; padding: 7px; border-radius: 5px;">
        <ul>
            <li>ƒê√°nh gi√° danh ti·∫øng th∆∞∆°ng hi·ªáu tr√™n th·ªã tr∆∞·ªùng.</li>
            <li>Theo d√µi, ƒë√°nh gi√° nh·∫±m c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªãch v·ª• v√† s·∫£n ph·∫©m.</li>
            <li>N√¢ng cao tr·∫£i nghi·ªám c·ªßa kh√°ch h√†ng.</li>
        </ul>
    </div>
    ''', unsafe_allow_html=True)
    st.write("##                              ")
    st.markdown('**3. C√°ch s·ª≠ d·ª•ng ·ª©ng d·ª•ng:**')
    st.warning('3.1. H√£y nh·∫≠p B√¨nh Lu·∫≠n b·∫•t k·ª≥ ƒë·ªÉ m√¥ h√¨nh d·ª± ƒëo√°n ƒë√°nh gi√°')
    bullet_list_html= """
            <p>
            <ul style="list-style-type: circle;">
                <li><strong> B√¨nh lu·∫≠n t√≠ch c·ª±c</strong> ü§ó</li> 
                <li><strong> B√¨nh lu·∫≠n ti√™u c·ª±c</strong> ü§Æ</li>
            </ul>
            """
    st.markdown(bullet_list_html, unsafe_allow_html=True)
    st.warning('3.2. H√£y nh·∫≠p ID Nh√† H√†ng ƒë·ªÉ ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng Nh√† H√†ng kh·∫£o s√°t')
    bullet_list_html2= """
            <p>
            <ul style="list-style-type: circle;">
                <li><strong> Th√¥ng tin d·ªØ li·ªáu kh·∫£o s√°t</strong></li> 
                <li><strong> ƒêi·ªÉm ƒë√°nh gi√° (Rating)</strong></li>
                <li><strong> Nh·∫≠n x√©t n·ªïi b·∫≠t v·ªÅ Nh√† H√†ng</strong></li>
            </ul>
            """
    st.markdown(bullet_list_html2, unsafe_allow_html=True)
    st.write("---")

    st.markdown('**4. H·ªçc vi√™n**')
    st.write("Nguy·ªÖn Th·ªã Kim Kh√°nh - Email: kimkhanh312@gmail.com")
    st.write("Nguy·ªÖn B√° ƒê√¨nh - Email: dinhnb1412@gmail.com")

elif choice == 'Th√¥ng Tin Thu·∫≠t To√°n':
    st.write("#### :blue[1. Dataset Exploration:]")
    st.write("Data source: Restaurants , Reviews")
    df_t = pd.read_csv('2_Reviews_head.csv')
    st.dataframe(df_t.head(5))
    shape="""Dataset "Reviews" includes <span style="color: #339966;"><strong>29,958 </strong> </span>rows and <span style="color: #339966;"><strong>6</strong></span> columns
             <p>
             """
    shape2="""Dataset "Restaurants" includes <span style="color: #339966;"><strong>1,622  </strong> </span>rows and <span style="color: #339966;"><strong>6</strong></span> columns
             <p>
             """
    st.markdown(shape, unsafe_allow_html=True)
    st.markdown(shape2, unsafe_allow_html=True)

    st.image('3.PNG', caption='M·ª©c ƒë·ªô ph√¢n b·ªë c·ªßa c√°c Nh√† H√†ng')
    st.image('2.jpg', caption='M·ª©c ƒë·ªô ph√¢n b·ªë rating c·ªßa Nh√† H√†ng')
    st.write("##### * Comment:")
    st.write("D·ª±a v√†o bi·ªÉu ƒë·ªì tr√™n ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng t·∫≠p d·ªØ li·ªáu n√†y c√≥ m·ªôt m·ª©c ƒë·ªô m·∫•t c√¢n b·∫±ng ƒë√°ng k·ªÉ gi·ªØa c√°c nh√£n rating.\
                V·ªõi kho·∫£ng rating 6-10 chi·∫øm 2/3 t·∫≠p d·ªØ li·ªáu v√† ch·ªâ 1/3 m·∫´u trong kho·∫£ng rating 0-6, ch√∫ng ta th·∫•y r·∫±ng c√≥ m·ªôt s·ª± ch√™nh l·ªách l·ªõn gi·ªØa hai nh√≥m n√†y. \
                ƒêi·ªÅu n√†y c√≥ th·ªÉ g√¢y ra c√°c v·∫•n ƒë·ªÅ khi ch√∫ng ta hu·∫•n luy·ªán m√¥ h√¨nh machine learning.")
    st.image('1.PNG', caption='So s√°nh th·ªùi gian v√† ƒë·ªô ch√≠nh x√°c c·ªßa c√°c thu·∫≠t to√°n')
    st.write("##                              ")
    st.write("#### :blue[2. Model Sentiment Analysis:]")
    st.subheader("Classification Report")
    json_data = """
    {
        "0": {
            "precision": 0.88,
            "recall": 0.94,
            "f1-score": 0.91,
            "support": 4516
        },
        "1": {
            "precision": 0.94,
            "recall": 0.87,
            "f1-score": 0.90,
            "support": 4509
        },
        "accuracy": {
            "precision": "",
            "recall": "",
            "f1-score": 0.91,
            "support": 9025
        },
        "macro avg": {
            "precision": 0.91,
            "recall": 0.91,
            "f1-score": 0.91,
            "support": 9025
        },
        "weighted avg": {
            "precision": 0.91,
            "recall": 0.91,
            "f1-score": 0.91,
            "support": 9025
        }
    }
    """

    # Load JSON data
    report_data = json.loads(json_data)
    data = {}
    for key, value in report_data.items():
        if isinstance(value, dict):
            data[key] = [value["precision"], value["recall"], value["f1-score"], value["support"]]
        else:
            data[key] = [value]  # ƒê·∫£m b·∫£o gi√° tr·ªã l√† m·ªôt danh s√°ch

    # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh DataFrame
    df = pd.DataFrame.from_dict(data, orient='index', columns=["precision", "recall", "f1-score", "support"])

    # Hi·ªÉn th·ªã DataFrame tr√™n Streamlit
    st.dataframe(df)

elif choice == 'ƒê√°nh Gi√° B√¨nh Lu·∫≠n':  
    style = """
    <style>
    div.row-widget.stRadio > div{font-size:30px !important;}
    </style>
    """
    st.markdown(style, unsafe_allow_html=True)
    #radio button
    btn = st.radio("Ch·ªçn 1 trong 2 ph∆∞∆°ng th·ª©c: üëá"
                   , ['Nh·∫≠p b√¨nh lu·∫≠n', 'Upload File']
                   )
    if btn == 'Nh·∫≠p b√¨nh lu·∫≠n':
        # map_vietnam = folium.Map(location=[10.809929141198806, 106.64572837501036], zoom_start=10)
        # map_vietnam.add_child(folium.LatLngPopup())
        # for index, row in df.iterrows():
        #     folium.Marker([row['latitude'], row['longitude']]).add_to(map_vietnam)

        # # # Hi·ªÉn th·ªã b·∫£n ƒë·ªì trong Streamlit
        # st.subheader('C√°c Nh√† H√†ng t·∫°i Th√†nh ph·ªë H·ªì Ch√≠ Minh')
        # folium_static(map_vietnam)
        title = st.text_input("Nh·∫≠p b√¨nh lu·∫≠n v·ªÅ Nh√† H√†ng ·ªü ƒë√¢y:")
        if len(title) > 0:
            output = predict(model_pkl_file, comment = title)
            if output[0] == 0 or output[0] == '0':
                st.write(f'B√¨nh lu·∫≠n "{title}" n√†y l√† ti√™u c·ª±c üòû')
            else:
                st.write(f'B√¨nh lu·∫≠n "{title}" n√†y l√† t√≠ch c·ª±c üòÄ')
    else:
        st.warning("Download Template (.csv):")
        df_temp = pd.read_csv("Template Input.csv")
        download_csv(df_temp, 'Template Upload.csv')
        st.warning("Upload File ƒë√£ nh·∫≠p b√¨nh lu·∫≠n:")
        uploaded_files = st.file_uploader("Upload File", accept_multiple_files=False, type=['csv'])
        df = None
        if uploaded_files is not None:
            st.success('File upload th√†nh c√¥ng!')
            df = pd.read_csv(uploaded_files)
            st.dataframe(df)
            flag = True 

            if 'Comment' in df.columns: 
                st.write("Processing...")
                df['D·ª± ƒëo√°n c·∫£m x√∫c'] = df['Comment'].apply(lambda x: 'T√≠ch c·ª±c üòÄ' if predict(model_pkl_file, comment=x) == 1 else 'Ti√™u c·ª±c üòû')
                st.write(" :green[ D·ª± ƒëo√°n ƒë√°nh gi√° c·∫£m x√∫c c·ªßa kh√°ch h√†ng: ] ")
                st.dataframe(df)
            else:
                st.error("File upload kh√¥ng ƒë√∫ng m·∫´u y√™u c·∫ßu")
            #download df
            download_csv(df, 'K·∫øt qu·∫£ d·ª± ƒëo√°n c·∫£m x√∫c.csv')

elif choice == 'ƒê√°nh Gi√° Nh√† H√†ng':  
    on = st.toggle('ID Restaurant')
    if on:
        map_vietnam = folium.Map(location=[10.809929141198806, 106.64572837501036], zoom_start=10, width=800, height=600)
        map_vietnam.add_child(folium.LatLngPopup())
        choose = st.radio('Ch·ªçn 1 trong 2 ph∆∞∆°ng th·ª©c: üëá'
                        , ['Nh·∫≠p ID Nh√† H√†ng', 'Ch·ªçn ID Nh√† H√†ng'])
        id = None
        if choose == 'Nh·∫≠p ID Nh√† H√†ng':
            id = get_id_input()
        else:
            try:
                id = st.selectbox('Vui l√≤ng ch·ªçn ID Nh√† H√†ng', tuple(df['ID']), index=14)
            except KeyError as e:
                st.warning('Vui l√≤ng ch·ªçn ƒë√∫ng ID nh√† h√†ng')
 
        result = {
            "ID" : id,
            "Name": df.loc[df['ID']==id]['Restaurant'],
            "Price": df.loc[df['ID']==id]['Price'],
            "Rating": str(round(df_comment[df_comment['IDRestaurant'] == id]['Rating'].mean(), 2)) + '/10'

        }
        data = pd.DataFrame(result)
        st.dataframe(data)

        folium.Marker([df.loc[id]['latitude'], df.loc[id]['longitude']]).add_to(map_vietnam)

        df_pos_sort, df_neg_sort = handle_comment_(df_comment, id)

        st.markdown("##### :blue[10 b√¨nh lu·∫≠n m·ªõi nh·∫•t v·ªÅ t√≠ch c·ª±c]")
        st.dataframe(df_pos_sort.head(10))
        st.markdown("##### :blue[10 b√¨nh lu·∫≠n m·ªõi nh·∫•t v·ªÅ ti√™u c·ª±c]")
        st.dataframe(df_neg_sort.head(10))

        rows = st.columns(2)
        try:
            text1 = ' '.join(df_pos_sort['Comment'])
            list_obj1 = filter_adjectives(text1)
            lst_pos = []
            for i in list_obj1:
                if i in positive_words:
                    lst_pos.append(i)
            if len(lst_pos) == 0:
                list_obj1 = ["No Positives"]
            # T·∫°o WordCloud cho comment t√≠ch c·ª±c
            wordcloud1 = WordCloud(width=400, height=400, random_state=42, background_color='white', max_words=20).generate(' '.join(lst_pos))
            
            # T·∫°o WordCloud cho comment ti√™u c·ª±c
            text2 = ' '.join(df_neg_sort['Comment'])
            list_obj2 = filter_adjectives(text2)

            lst_neg = []
            for i in list_obj2:
                if i in negative_words:
                    lst_neg.append(i)
            if len(list_obj1) == 0:
                list_obj1 = ["No Negatives"]
            wordcloud2 = WordCloud(width=400, height=400, random_state=42, background_color='white', max_words=20).generate(' '.join(lst_neg))

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
            ax1.imshow(wordcloud1, interpolation='bilinear')
            ax1.axis("off")
            ax1.set_title('WordCloud Positive')

            ax2.imshow(wordcloud2, interpolation='bilinear')
            ax2.axis("off")
            ax2.set_title('WordCloud Negative')
            st.pyplot(fig)

        except Exception as e:
            pass
        folium_static(map_vietnam)


    

